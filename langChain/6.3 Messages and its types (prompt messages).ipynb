{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf92be60-c5ab-448d-ab13-3ec34423c249",
   "metadata": {},
   "source": [
    "#### System message\n",
    "This is a behind‑the‑scenes instruction that tells the AI how to behave. It sets the rules, tone, and any special context before the conversation starts, so every later reply follows the guidelines it gives.\n",
    "\n",
    "#### Human message\n",
    "This is whatever the person (you) types next. It carries the actual question, request, or information the user wants the AI to respond to, exactly as written.\n",
    "\n",
    "#### AI message\n",
    "This is the AI’s reply. It uses the system instructions plus the latest human message (and conversation history) to generate a helpful, relevant answer and keep the dialogue moving.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4258ddbc-f094-4d29-b97e-ec29bedd93f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe44246-2b76-4513-be34-c41e6c0fb28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e96ff01-fd65-4061-b3c6-627437f24781",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "978b6e4b-24f0-46d7-80ec-fa736813564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(\n",
    "    api_key=api_key,\n",
    "    verbose=True,\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.7,\n",
    "    timeout=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35a34d50-e4bd-4822-b57b-09615673d372",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistent\"),\n",
    "    HumanMessage(content=\"Tell me about LangChain\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0287fb66-e162-40da-ab3d-5e2cd07fbca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a025fa8-eec0-4012-adc5-311ae9d96fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(AIMessage(content=result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f31849f-38c4-4975-bf33-7b3ab3b0f04f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Okay, let\\'s dive into LangChain!\\n\\n**What is LangChain?**\\n\\nLangChain is a framework designed to simplify the development of applications that use large language models (LLMs). Think of it as a toolkit that provides building blocks and abstractions to help you connect LLMs to other data sources and components, allowing you to create more sophisticated and powerful AI applications.\\n\\n**Key Concepts and Components:**\\n\\n*   **Models:** LangChain supports a wide variety of LLMs, including:\\n    *   OpenAI models (GPT-3, GPT-4, etc.)\\n    *   Hugging Face models\\n    *   Google\\'s PaLM\\n    *   Cohere\\n    *   And many others.\\n    *   It provides a standard interface for interacting with these models, making it easier to switch between them or experiment with different options.\\n\\n*   **Prompts:**  Prompts are the input you give to the LLM. LangChain helps you manage and optimize prompts through:\\n    *   **Prompt Templates:** Reusable templates for generating prompts, allowing you to inject variables and ensure consistency.\\n    *   **Example Selectors:** Dynamically choose the best examples to include in your prompt based on the input, improving the LLM\\'s performance.\\n\\n*   **Chains:** Chains are sequences of calls to LLMs or other utilities. They allow you to create more complex workflows, such as:\\n    *   **LLMChain:** A simple chain that combines a prompt template with an LLM.\\n    *   **SequentialChain:**  Executes multiple chains in a specific order, passing the output of one chain as input to the next.\\n    *   **RouterChain:**  Dynamically routes the input to different chains based on its content.\\n\\n*   **Indexes:** LangChain helps you structure your data so that LLMs can access and use it effectively. This involves:\\n    *   **Document Loaders:** Load data from various sources (text files, PDFs, websites, databases, etc.).\\n    *   **Text Splitters:** Divide large documents into smaller chunks that fit within the LLM\\'s context window.\\n    *   **Vectorstores:** Store text embeddings (numerical representations of text) for efficient retrieval. Common vectorstores include FAISS, Chroma, Pinecone, and Weaviate.\\n\\n*   **Memory:**  LLMs are typically stateless, meaning they don\\'t remember previous interactions. LangChain provides memory components to add state to your applications:\\n    *   **ConversationBufferMemory:** Stores the entire conversation history.\\n    *   **ConversationSummaryMemory:** Summarizes the conversation history to keep it concise.\\n    *   **ConversationBufferWindowMemory:** Stores a fixed number of recent interactions.\\n\\n*   **Agents:** Agents use LLMs to decide which actions to take. They can access various tools and use them to answer questions, gather information, or perform tasks. Key components include:\\n    *   **Tools:**  Functions that agents can use to interact with the outside world (e.g., search the web, run a calculator, access a database).\\n    *   **Toolkits:** Collections of tools that are designed to work together.\\n    *   **Agent Types:** Different strategies for how the agent decides which tool to use.\\n\\n*   **Callbacks:** Provides a standard interface for hooking into the execution of any LangChain component.  Useful for logging, monitoring, and debugging.\\n\\n**Why Use LangChain?**\\n\\n*   **Abstraction and Simplification:**  LangChain provides high-level abstractions that make it easier to work with LLMs, reducing boilerplate code.\\n*   **Modularity and Composability:**  Components are designed to be modular and composable, allowing you to easily create custom workflows.\\n*   **Integration:**  LangChain integrates with a wide range of LLMs, data sources, and tools, giving you flexibility in building your applications.\\n*   **Rapid Prototyping:**  The framework\\'s components and tools facilitate rapid prototyping and experimentation.\\n*   **Community and Ecosystem:**  LangChain has a growing community and ecosystem of developers and contributors, providing support and resources.\\n\\n**Common Use Cases:**\\n\\n*   **Question Answering:**  Build chatbots that can answer questions based on your data.\\n*   **Text Summarization:**  Summarize long documents or articles.\\n*   **Code Generation:**  Generate code from natural language descriptions.\\n*   **Data Extraction:** Extract information from unstructured text.\\n*   **Chatbots:** Create conversational agents that can engage in natural language interactions.\\n*   **Agent-Based Systems:** Build autonomous agents that can perform tasks using various tools.\\n\\n**Example (Simplified):**\\n\\n```python\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import LLMChain\\n\\n# 1. Initialize the LLM\\nllm = OpenAI(api_key=\"YOUR_API_KEY\")\\n\\n# 2. Create a prompt template\\nprompt_template = PromptTemplate(\\n    input_variables=[\"product\"],\\n    template=\"What is a good name for a company that makes {product}?\",\\n)\\n\\n# 3. Create an LLMChain\\nchain = LLMChain(llm=llm, prompt=prompt_template)\\n\\n# 4. Run the chain\\nproduct = \"eco-friendly cleaning supplies\"\\ncompany_name = chain.run(product)\\n\\nprint(f\"Company name: {company_name}\")\\n```\\n\\n**Getting Started:**\\n\\n1.  **Installation:** `pip install langchain`\\n2.  **API Keys:** You\\'ll need API keys for the LLMs you want to use (e.g., OpenAI API key).\\n3.  **Documentation:** The official LangChain documentation is a great resource: [https://python.langchain.com/](https://python.langchain.com/)\\n4.  **Tutorials and Examples:**  Explore the LangChain tutorials and examples to learn how to use the framework.\\n\\n**In Summary:**\\n\\nLangChain is a powerful framework that simplifies the development of LLM-powered applications. It provides a set of tools and abstractions that allow you to connect LLMs to other data sources and components, enabling you to create more sophisticated and useful AI solutions.  It\\'s a rapidly evolving project, so keep an eye on the latest updates and features.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "253b3686-484e-4d0d-b5bd-5daaf25798aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistent', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Tell me about LangChain', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Okay, let\\'s dive into LangChain!\\n\\n**What is LangChain?**\\n\\nLangChain is a framework designed to simplify the development of applications that use large language models (LLMs). Think of it as a toolkit that provides building blocks and abstractions to help you connect LLMs to other data sources and components, allowing you to create more sophisticated and powerful AI applications.\\n\\n**Key Concepts and Components:**\\n\\n*   **Models:** LangChain supports a wide variety of LLMs, including:\\n    *   OpenAI models (GPT-3, GPT-4, etc.)\\n    *   Hugging Face models\\n    *   Google\\'s PaLM\\n    *   Cohere\\n    *   And many others.\\n    *   It provides a standard interface for interacting with these models, making it easier to switch between them or experiment with different options.\\n\\n*   **Prompts:**  Prompts are the input you give to the LLM. LangChain helps you manage and optimize prompts through:\\n    *   **Prompt Templates:** Reusable templates for generating prompts, allowing you to inject variables and ensure consistency.\\n    *   **Example Selectors:** Dynamically choose the best examples to include in your prompt based on the input, improving the LLM\\'s performance.\\n\\n*   **Chains:** Chains are sequences of calls to LLMs or other utilities. They allow you to create more complex workflows, such as:\\n    *   **LLMChain:** A simple chain that combines a prompt template with an LLM.\\n    *   **SequentialChain:**  Executes multiple chains in a specific order, passing the output of one chain as input to the next.\\n    *   **RouterChain:**  Dynamically routes the input to different chains based on its content.\\n\\n*   **Indexes:** LangChain helps you structure your data so that LLMs can access and use it effectively. This involves:\\n    *   **Document Loaders:** Load data from various sources (text files, PDFs, websites, databases, etc.).\\n    *   **Text Splitters:** Divide large documents into smaller chunks that fit within the LLM\\'s context window.\\n    *   **Vectorstores:** Store text embeddings (numerical representations of text) for efficient retrieval. Common vectorstores include FAISS, Chroma, Pinecone, and Weaviate.\\n\\n*   **Memory:**  LLMs are typically stateless, meaning they don\\'t remember previous interactions. LangChain provides memory components to add state to your applications:\\n    *   **ConversationBufferMemory:** Stores the entire conversation history.\\n    *   **ConversationSummaryMemory:** Summarizes the conversation history to keep it concise.\\n    *   **ConversationBufferWindowMemory:** Stores a fixed number of recent interactions.\\n\\n*   **Agents:** Agents use LLMs to decide which actions to take. They can access various tools and use them to answer questions, gather information, or perform tasks. Key components include:\\n    *   **Tools:**  Functions that agents can use to interact with the outside world (e.g., search the web, run a calculator, access a database).\\n    *   **Toolkits:** Collections of tools that are designed to work together.\\n    *   **Agent Types:** Different strategies for how the agent decides which tool to use.\\n\\n*   **Callbacks:** Provides a standard interface for hooking into the execution of any LangChain component.  Useful for logging, monitoring, and debugging.\\n\\n**Why Use LangChain?**\\n\\n*   **Abstraction and Simplification:**  LangChain provides high-level abstractions that make it easier to work with LLMs, reducing boilerplate code.\\n*   **Modularity and Composability:**  Components are designed to be modular and composable, allowing you to easily create custom workflows.\\n*   **Integration:**  LangChain integrates with a wide range of LLMs, data sources, and tools, giving you flexibility in building your applications.\\n*   **Rapid Prototyping:**  The framework\\'s components and tools facilitate rapid prototyping and experimentation.\\n*   **Community and Ecosystem:**  LangChain has a growing community and ecosystem of developers and contributors, providing support and resources.\\n\\n**Common Use Cases:**\\n\\n*   **Question Answering:**  Build chatbots that can answer questions based on your data.\\n*   **Text Summarization:**  Summarize long documents or articles.\\n*   **Code Generation:**  Generate code from natural language descriptions.\\n*   **Data Extraction:** Extract information from unstructured text.\\n*   **Chatbots:** Create conversational agents that can engage in natural language interactions.\\n*   **Agent-Based Systems:** Build autonomous agents that can perform tasks using various tools.\\n\\n**Example (Simplified):**\\n\\n```python\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import LLMChain\\n\\n# 1. Initialize the LLM\\nllm = OpenAI(api_key=\"YOUR_API_KEY\")\\n\\n# 2. Create a prompt template\\nprompt_template = PromptTemplate(\\n    input_variables=[\"product\"],\\n    template=\"What is a good name for a company that makes {product}?\",\\n)\\n\\n# 3. Create an LLMChain\\nchain = LLMChain(llm=llm, prompt=prompt_template)\\n\\n# 4. Run the chain\\nproduct = \"eco-friendly cleaning supplies\"\\ncompany_name = chain.run(product)\\n\\nprint(f\"Company name: {company_name}\")\\n```\\n\\n**Getting Started:**\\n\\n1.  **Installation:** `pip install langchain`\\n2.  **API Keys:** You\\'ll need API keys for the LLMs you want to use (e.g., OpenAI API key).\\n3.  **Documentation:** The official LangChain documentation is a great resource: [https://python.langchain.com/](https://python.langchain.com/)\\n4.  **Tutorials and Examples:**  Explore the LangChain tutorials and examples to learn how to use the framework.\\n\\n**In Summary:**\\n\\nLangChain is a powerful framework that simplifies the development of LLM-powered applications. It provides a set of tools and abstractions that allow you to connect LLMs to other data sources and components, enabling you to create more sophisticated and useful AI solutions.  It\\'s a rapidly evolving project, so keep an eye on the latest updates and features.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04096dd-6469-4a44-9d30-4bd8c3f2f2b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1d1346-6724-4bcb-9454-cae83ccaa86c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf4c8050-1219-42bf-a046-85c43433f063",
   "metadata": {},
   "source": [
    "# Chat Bot with History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17f20c1c-83e2-46eb-98a4-8dfe2ef4316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3ecb218-29e7-4c9d-a243-6ec2bf90218a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5b4e70-fbd8-48d1-a9ef-b54b4c4f0e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25f38728-6059-4a5d-9bca-a8bc642c8c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ[\"GEMINI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a79d9-7b7e-4aed-9c60-fc9fd1561832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5ff15f8-6cec-4bf7-b78c-cb649edb0095",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(\n",
    "    api_key=api_key,\n",
    "    verbose=True,\n",
    "    model = 'gemini-2.0-flash',\n",
    "    timeout=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4dd4fb7-d3bf-4e0c-8559-ab7a8939f113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--87fee34c-069d-402b-9195-8bfe3f81d6d0-0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1252e2a-4871-4973-bfe7-2a7a2aa7ad28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11306957-5d3f-4a22-871e-a6c331f5a0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c87b2f6-6842-403b-9ad9-614f0c2c646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a general help provider, who answer in short and in simple language\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2173e33a-7f46-4495-a5a5-f7a8baebac30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "312292b9-c0d7-44d5-bc48-b372d78cf68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  what is age of ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MS can be diagnosed at any age, but it's most common between 20 and 50.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  msdhoni\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MS Dhoni is an Indian cricketer. He was born on July 7, 1981. So, as of today (October 26, 2023), he is **42 years old**.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  what is his wife name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "His wife's name is Sakshi Dhoni.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  ok\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! Is there anything else I can help you with?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  what is my name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know your name. I'm a computer program and don't have access to that kind of personal information. You haven't told me your name.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  i am mohit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, Mohit! Nice to meet you. How can I help you today, Mohit?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  nothing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright, Mohit. If you need anything in the future, just ask! Have a great day.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  sure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! Have a good one, Mohit!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  dont reply me, for this message\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understood. I will not reply to this message.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  then why you reply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're right! My apologies. I was still processing the initial instruction. I will now stop replying.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  ok\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Silence)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  exit\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user = input(\"User: \")\n",
    "    if user == 'exit':\n",
    "        break\n",
    "\n",
    "    messages.append(HumanMessage(content=user))\n",
    "    response = model.invoke(messages)\n",
    "    messages.append(AIMessage(content=response.content))\n",
    "    print(response.content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d200ca1-6b11-405b-9b63-fb94653dd521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a general help provider, who answer in short and in simple language', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hello! How can I help you today?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is age of ms', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"MS can be diagnosed at any age, but it's most common between 20 and 50.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='msdhoni', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='MS Dhoni is an Indian cricketer. He was born on July 7, 1981. So, as of today (October 26, 2023), he is **42 years old**.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is his wife name', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"His wife's name is Sakshi Dhoni.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='ok', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Great! Is there anything else I can help you with?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is my name', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I don't know your name. I'm a computer program and don't have access to that kind of personal information. You haven't told me your name.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='i am mohit', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Okay, Mohit! Nice to meet you. How can I help you today, Mohit?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='nothing', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Alright, Mohit. If you need anything in the future, just ask! Have a great day.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='sure', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Great! Have a good one, Mohit!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='dont reply me, for this message', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Understood. I will not reply to this message.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='then why you reply', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"You're right! My apologies. I was still processing the initial instruction. I will now stop replying.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='ok', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='(Silence)', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a596940f-ba59-463a-bb7b-62948bd44ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be79b8f-c4c2-4d2b-a578-3af5f54655cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a7065-a769-458e-9754-47333cd37701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_langchain",
   "language": "python",
   "name": ".venv_langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
