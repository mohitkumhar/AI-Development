{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a30c09-cfeb-4b7f-84bc-0d80e4732382",
   "metadata": {},
   "source": [
    "# <center>Chains\n",
    "- Chains are a way to connect different steps in a process to make the AI do more complex tasks.\n",
    "- Each step can be something like using a prompt, calling a language model (like GPT), using a tool, or saving memory.\n",
    "- LangChain helps you link these steps together to build smart AI apps like chatbots, question-answer systems, or summarizers.\n",
    "\n",
    "## Types of Chains -\n",
    "- 1. Simple Chain\n",
    "- 2. Sequential Chain\n",
    "- 3. Conditional Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dc2b7e-40ad-4fca-96fa-588c1fb44630",
   "metadata": {},
   "source": [
    "# <center>1. Simple Chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b064890-0f00-4c73-8031-95f2d6d5a31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f4362c3-0a20-4ab4-87da-294c5adec04d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0176b780-c910-40d9-b624-e36403a61f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate(\n",
    "    template='Geenrate 5 interesting facts about {topic}',\n",
    "    input_variables=['topic'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dedcb81f-b6f6-4663-8f67-cd5120ef71b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(api_key=os.environ['GEMINI_API_KEY'], model='gemini-2.0-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20afc9d2-3558-4a13-9543-2ae8ad656619",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65412941-c9b5-4015-9e2c-683275475d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = template | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c3bda44-1a87-45a7-805f-683872c11361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Geenrate 5 interesting facts about {topic}')\n",
       "| ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001AD6E9A38E0>, default_metadata=(), model_kwargs={})\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05843919-f15c-4d7e-b507-68c1051c5f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"topic\": \"game\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "506352a1-5ae9-4df3-bbb2-b3edc0b070e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Okay, here are 5 interesting facts about the concept of \"games\" in general, covering different aspects:\\n\\n1.  **Games predate written language:** Archaeological evidence suggests that games have been played for thousands of years, with some games like Mancala dating back to 5000-7000 BC. This means humans were entertaining themselves with games long before they developed writing systems.\\n\\n2.  **Games are a powerful tool for learning:**  While often seen as frivolous, games can be incredibly effective educational tools. They provide a risk-free environment to experiment, make mistakes, and learn from them. Studies have shown that games can improve problem-solving skills, critical thinking, and even memory.\\n\\n3.  **The highest-grossing video game of all time is not what you might expect:** While titles like Grand Theft Auto V and Call of Duty are huge, the highest-grossing video game of all time is actually a free-to-play online battle arena game called \"Honor of Kings\" (王者荣耀), primarily popular in China.\\n\\n4.  **The first video game console was called the Magnavox Odyssey:** Released in 1972, the Magnavox Odyssey predates Atari and was the first home video game console. It used simple analog circuitry and overlays for the screen to simulate different games.\\n\\n5.  **Games can have therapeutic benefits:**  Video games are increasingly being recognized for their therapeutic potential. They can be used to treat anxiety, depression, PTSD, and ADHD. Games can also help with physical rehabilitation, pain management, and cognitive training.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7e4fde-a7f4-4b44-990e-a7ac37850874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1c5e9dc-b414-4d33-a188-455c548282ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      +-------------+      \n",
      "      | PromptInput |      \n",
      "      +-------------+      \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "+------------------------+ \n",
      "| ChatGoogleGenerativeAI | \n",
      "+------------------------+ \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "    +-----------------+    \n",
      "    | StrOutputParser |    \n",
      "    +-----------------+    \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81420540-7ef0-4cfc-8702-5f98d1a91576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab52ecb7-e605-4b8f-b294-079c8de82e8e",
   "metadata": {},
   "source": [
    "# <center>2. Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8747c173-8f0f-4499-a633-e9d410d0fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87737b8a-463c-41a6-9618-89a51bdb2ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33657ebb-d4ab-41e1-9dd9-46e93e48701f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1673b78-c3ca-4833-8697-3009846b8edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.environ['GEMINI_API_KEY']\n",
    "MODEL_NAME = 'gemini-2.0-flash'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba47bc60-464a-4a39-ad1e-89297f74a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(api_key=API_KEY, model=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "270512e6-ca38-4b06-8445-8c10bb8f4a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "725504d9-e04c-42b6-a2ce-3c11bc124e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = PromptTemplate(\n",
    "    template=\"write me detail info about topic {topic}\",\n",
    "    input_variables=['topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44da7a73-260e-4dd7-9aea-56ab15455287",
   "metadata": {},
   "outputs": [],
   "source": [
    "template2 = PromptTemplate(\n",
    "    template=\"write me 5 line short summary on this detail information: {detail_information}\",\n",
    "    input_variables=['detail_information']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55575026-7015-4f58-8aed-57e02dd587fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = template1 | model | parser | template2 | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5d4c7c9-6f19-4fcc-9439-b97330ad9de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"topic\": \"laptop\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f39f6156-a153-4692-a466-8e6f5716a668",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Laptops are portable personal computers integrating all desktop components into a single unit. Their history spans from early prototypes to modern Ultrabooks and 2-in-1s, driven by advancements in processing power, storage, and display technology. Key components include CPU, GPU, RAM, storage (HDD/SSD), display, and ports. Laptops offer portability and convenience but can be pricier and less upgradeable than desktops. Future trends include foldable screens, AI integration, and improved battery technology.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bfe00dd-549a-4947-9e0a-cd9aea4e4da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      +-------------+      \n",
      "      | PromptInput |      \n",
      "      +-------------+      \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "+------------------------+ \n",
      "| ChatGoogleGenerativeAI | \n",
      "+------------------------+ \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "    +-----------------+    \n",
      "    | StrOutputParser |    \n",
      "    +-----------------+    \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "+------------------------+ \n",
      "| ChatGoogleGenerativeAI | \n",
      "+------------------------+ \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "    +-----------------+    \n",
      "    | StrOutputParser |    \n",
      "    +-----------------+    \n",
      "             *             \n",
      "             *             \n",
      "             *             \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85737ac8-4a6f-4b84-a9f1-ea66ef9b4c23",
   "metadata": {},
   "source": [
    "# <center>Parallel Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "18e7a055-6888-4c2f-9c7a-10b4e73900c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1f83381d-e633-4da0-852d-261909a456e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d27e2e3c-d4fe-4889-960b-8e9b2fdb22d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ec7729ce-524e-40e0-8eef-79099cf5af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ['GEMINI_API_KEY']\n",
    "model = 'gemini-2.0-flash'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "05c6fb3a-4cfb-47dc-ad88-f8430e60f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(api_key=api_key, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d0bc7b86-5c1e-4c6a-a3b2-43eac39a07de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = ChatAnthropic(api_key=os.environ['ANTHROPIC_API_KEY'], model='claude-3-7-sonnet-20250219')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c240a4ea-fa89-4f9b-a9c6-a92a87290fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = PromptTemplate(\n",
    "    template=\"Generate short and simple notes from the following text \\n {text}\",\n",
    "    input_variables=['text'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f6df1e76-f0ce-4a5e-9eb6-fa00df0faadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = PromptTemplate(\n",
    "    template='Generate 5 short question answer from the following text \\m {text}',\n",
    "    input_variables=['text'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "003e8682-0f89-4712-a2d9-de9d75af7288",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3 = PromptTemplate(\n",
    "    template=\"Merge the provided notes and quiz into a single document \\n notes -> {notes}, quiz -> {quiz}\",\n",
    "    input_variables=['notes', 'quiz']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2d7525-03bd-420c-816c-e17f4291d29c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4e5e26b7-8d5f-4852-b4e6-dcdad37f3f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7a915965-132a-4ad8-acc6-66007f5c21a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_chain = RunnableParallel({\n",
    "    'notes': prompt1 | model | parser,\n",
    "    'quiz': prompt2 | model | parser,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "23e0b86e-0370-46a9-9246-5ff3325a1669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  notes: PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='Generate short and simple notes from the following text \\n {text}')\n",
       "         | ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001AD07257070>, default_metadata=(), model_kwargs={})\n",
       "         | StrOutputParser(),\n",
       "  quiz: PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='Generate 5 short question answer from the following text \\\\m {text}')\n",
       "        | ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001AD07257070>, default_metadata=(), model_kwargs={})\n",
       "        | StrOutputParser()\n",
       "}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "deb9d37f-39f6-49d6-b6cd-6230ba1081e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_chain = prompt3 | model | parser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9ad77cac-54e2-414e-9628-23628df24209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['notes', 'quiz'], input_types={}, partial_variables={}, template='Merge the provided notes and quiz into a single document \\n notes -> {notes}, quiz -> {quiz}')\n",
       "| ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001AD07257070>, default_metadata=(), model_kwargs={})\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a9fab729-e5ba-4329-a44f-c34325e05fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = parallel_chain | merge_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6a9ebc29-50bd-4e94-ba92-190cc5c4c843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  notes: PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='Generate short and simple notes from the following text \\n {text}')\n",
       "         | ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001AD07257070>, default_metadata=(), model_kwargs={})\n",
       "         | StrOutputParser(),\n",
       "  quiz: PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='Generate 5 short question answer from the following text \\\\m {text}')\n",
       "        | ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001AD07257070>, default_metadata=(), model_kwargs={})\n",
       "        | StrOutputParser()\n",
       "}\n",
       "| PromptTemplate(input_variables=['notes', 'quiz'], input_types={}, partial_variables={}, template='Merge the provided notes and quiz into a single document \\n notes -> {notes}, quiz -> {quiz}')\n",
       "| ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001AD07257070>, default_metadata=(), model_kwargs={})\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a6432a1e-3610-4d52-bfc9-5d546546d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"1.4.1.2. Scores and probabilities\n",
    "The decision_function method of SVC and NuSVC gives per-class scores for each sample (or a single score per sample in the binary case). When the constructor option probability is set to True, class membership probability estimates (from the methods predict_proba and predict_log_proba) are enabled. In the binary case, the probabilities are calibrated using Platt scaling [9]: logistic regression on the SVM’s scores, fit by an additional cross-validation on the training data. In the multiclass case, this is extended as per [10].\n",
    "\n",
    "Note\n",
    "\n",
    "The same probability calibration procedure is available for all estimators via the CalibratedClassifierCV (see Probability calibration). In the case of SVC and NuSVC, this procedure is builtin to libsvm which is used under the hood, so it does not rely on scikit-learn’s CalibratedClassifierCV.\n",
    "\n",
    "The cross-validation involved in Platt scaling is an expensive operation for large datasets. In addition, the probability estimates may be inconsistent with the scores:\n",
    "\n",
    "the “argmax” of the scores may not be the argmax of the probabilities\n",
    "\n",
    "in binary classification, a sample may be labeled by predict as belonging to the positive class even if the output of predict_proba is less than 0.5; and similarly, it could be labeled as negative even if the output of predict_proba is more than 0.5.\n",
    "\n",
    "Platt’s method is also known to have theoretical issues. If confidence scores are required, but these do not have to be probabilities, then it is advisable to set probability=False and use decision_function instead of predict_proba.\n",
    "\n",
    "Please note that when decision_function_shape='ovr' and n_classes > 2, unlike decision_function, the predict method does not try to break ties by default. You can set break_ties=True for the output of predict to be the same as np.argmax(clf.decision_function(...), axis=1), otherwise the first class among the tied classes will always be returned; but have in mind that it comes with a computational cost. See SVM Tie Breaking Example for an example on tie breaking.\n",
    "\n",
    "1.4.1.3. Unbalanced problems\n",
    "In problems where it is desired to give more importance to certain classes or certain individual samples, the parameters class_weight and sample_weight can be used.\n",
    "\n",
    "SVC (but not NuSVC) implements the parameter class_weight in the fit method. It’s a dictionary of the form {class_label : value}, where value is a floating point number > 0 that sets the parameter C of class class_label to C * value. The figure below illustrates the decision boundary of an unbalanced problem, with and without weight correction.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "90c86927-6693-49f6-a63b-cc1312226bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"text\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "00ab1c0c-486d-4dc3-b796-e646d5cf4d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, here's the merged document combining the notes and quiz questions & answers.\\n\\n**Support Vector Machines (SVM) - Scores, Probabilities, and Unbalanced Problems**\\n\\n**1.4.1.2 Scores and Probabilities:**\\n\\n*   `decision_function`: Provides per-class scores (binary: single score).\\n*   `probability=True`: Enables probability estimates (`predict_proba`, `predict_log_proba`).\\n    *   Binary case: Platt scaling (logistic regression on SVM scores, cross-validation).\\n    *   Multiclass case: Extension of Platt scaling.\\n*   **Note:** Platt scaling is computationally expensive and may lead to inconsistencies:\\n    *   `argmax(scores)` might not equal `argmax(probabilities)`.\\n    *   In binary, `predict` and `predict_proba` outputs can disagree around 0.5.\\n*   If probabilities aren't strictly needed, use `decision_function` with `probability=False` for confidence scores.\\n*   `decision_function_shape='ovr'` and `n_classes > 2`, `predict` does not break ties by default. Set `break_ties=True` for same result as `np.argmax(clf.decision_function(...), axis=1)`.\\n\\n**1.4.1.3 Unbalanced Problems:**\\n\\n*   `class_weight`, `sample_weight`: Give more importance to specific classes/samples.\\n*   `SVC` (not `NuSVC`): `class_weight` in `fit` method.\\n    *   Dictionary: `{class_label: value}`.  `C` for `class_label` becomes `C * value`.\\n\\n**Quiz: Testing Your Knowledge**\\n\\n**1. Question:** What method provides per-class scores for each sample in SVC and NuSVC?\\n**Answer:** The `decision_function` method.\\n\\n**2. Question:** What happens when the `probability` constructor option is set to `True` in SVC and NuSVC?\\n**Answer:** Class membership probability estimates (from the methods `predict_proba` and `predict_log_proba`) are enabled.\\n\\n**3. Question:** What is used to calibrate probabilities in the binary case for SVC and NuSVC?\\n**Answer:** Platt scaling (logistic regression on the SVM's scores).\\n\\n**4. Question:** What are two parameters that can be used to give more importance to certain classes or samples in SVC?\\n**Answer:** `class_weight` and `sample_weight`.\\n\\n**5. Question:** Which SVC method allows you to specify class weights?\\n**Answer:** The `fit` method.\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bb7c72e0-899d-4e95-be14-9578a7d30e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    +---------------------------+                      \n",
      "                    | Parallel<notes,quiz>Input |                      \n",
      "                    +---------------------------+                      \n",
      "                       ***                   ***                       \n",
      "                   ****                         ****                   \n",
      "                 **                                 **                 \n",
      "    +----------------+                          +----------------+     \n",
      "    | PromptTemplate |                          | PromptTemplate |     \n",
      "    +----------------+                          +----------------+     \n",
      "             *                                           *             \n",
      "             *                                           *             \n",
      "             *                                           *             \n",
      "+------------------------+                  +------------------------+ \n",
      "| ChatGoogleGenerativeAI |                  | ChatGoogleGenerativeAI | \n",
      "+------------------------+                  +------------------------+ \n",
      "             *                                           *             \n",
      "             *                                           *             \n",
      "             *                                           *             \n",
      "    +-----------------+                         +-----------------+    \n",
      "    | StrOutputParser |                         | StrOutputParser |    \n",
      "    +-----------------+                         +-----------------+    \n",
      "                       ***                   ***                       \n",
      "                          ****           ****                          \n",
      "                              **       **                              \n",
      "                    +----------------------------+                     \n",
      "                    | Parallel<notes,quiz>Output |                     \n",
      "                    +----------------------------+                     \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                          +----------------+                           \n",
      "                          | PromptTemplate |                           \n",
      "                          +----------------+                           \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                      +------------------------+                       \n",
      "                      | ChatGoogleGenerativeAI |                       \n",
      "                      +------------------------+                       \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                          +-----------------+                          \n",
      "                          | StrOutputParser |                          \n",
      "                          +-----------------+                          \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                                   *                                   \n",
      "                      +-----------------------+                        \n",
      "                      | StrOutputParserOutput |                        \n",
      "                      +-----------------------+                        \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085520e6-4dfa-4d56-b046-d86a1dd96d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff229aab-2ad5-4eab-82eb-a3bbdcf4e010",
   "metadata": {},
   "source": [
    "# <center>Conditional Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e5251b6e-b463-426d-bcb4-8c7b9424ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from typing import Literal\n",
    "from langchain.schema.runnable import RunnableBranch, RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9795a84b-c98a-4d8f-b366-111755aba3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0bbf396e-685a-484d-9113-002ba58bee20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f3267ff7-3a44-4234-b8ff-ff401b049dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GoogleGenerativeAI(api_key=os.environ['GEMINI_API_KEY'], model='gemini-2.0-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b0412c61-95c9-4beb-a08c-5d44cd91e198",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6cbb1d44-7c43-4157-9a2b-26754b12f533",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedback(BaseModel):\n",
    "    sentiment: Literal[\"positive\", \"negative\"] = Field(description=\"Give the sentiment of the feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "34d476af-a58f-4b75-b8ea-3c9956d6112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser2 = PydanticOutputParser(pydantic_object=Feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "296b5830-145d-4556-a591-7ae4984c0e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = PromptTemplate(\n",
    "    template=\"Classify the sentiment of the following feedback text into positive or negative \\n {feedback} \\n {format_instruction}\",\n",
    "    input_variables=[\"feedback\"],\n",
    "    partial_variables={\"format_instruction\": parser2.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3b9bb226-b316-4213-895d-b1b49bf9ef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_chain = prompt1 | model | parser2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "dfd0ed51-c270-4e79-b2e0-b42319dab010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['feedback'], input_types={}, partial_variables={'format_instruction': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"sentiment\": {\"description\": \"Give the sentiment of the feedback\", \"enum\": [\"positive\", \"negative\"], \"title\": \"Sentiment\", \"type\": \"string\"}}, \"required\": [\"sentiment\"]}\\n```'}, template='Classify the sentiment of the following feedback text into positive or negative \\n {feedback} \\n {format_instruction}')\n",
       "| GoogleGenerativeAI(model='gemini-2.0-flash', google_api_key=SecretStr('**********'), client=ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001AD0CA72CB0>, default_metadata=(), model_kwargs={}))\n",
       "| PydanticOutputParser(pydantic_object=<class '__main__.Feedback'>)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e3333c62-8a84-4c76-8580-7f9eaedad9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = PromptTemplate(\n",
    "    template = \"Write an appropriate response to this positive feedback \\n {feedback}\",\n",
    "    input_variable=['feedback'], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8bb7a19e-18ac-461d-aa9b-6e2bbd3208fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3 = PromptTemplate(\n",
    "    template=\"Write an appropriate response to this negative feedback \\n {feedback}\",\n",
    "    input_variable=['feedback']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "94706159-029e-4cd0-a7c8-8a0193a17957",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_chain = RunnableBranch(\n",
    "    # (condition1, chain)\n",
    "    (lambda x: x.sentiment == 'positive', prompt2 | model | parser),\n",
    "    (lambda x: x.sentiment == 'negative', prompt3 | model | parser),\n",
    "    \n",
    "    # default chain\n",
    "    RunnableLambda(lambda x: \"cound not find any sentiment\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "12e527c2-8f19-4481-9f23-6b8a39db3995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBranch(branches=[(RunnableLambda(lambda x: x.sentiment == 'positive'), PromptTemplate(input_variables=['feedback'], input_types={}, partial_variables={}, template='Write an appropriate response to this positive feedback \\n {feedback}')\n",
       "| GoogleGenerativeAI(model='gemini-2.0-flash', google_api_key=SecretStr('**********'), client=ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001AD0CA72CB0>, default_metadata=(), model_kwargs={}))\n",
       "| StrOutputParser()), (RunnableLambda(lambda x: x.sentiment == 'negative'), PromptTemplate(input_variables=['feedback'], input_types={}, partial_variables={}, template='Write an appropriate response to this negative feedback \\n {feedback}')\n",
       "| GoogleGenerativeAI(model='gemini-2.0-flash', google_api_key=SecretStr('**********'), client=ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001AD0CA72CB0>, default_metadata=(), model_kwargs={}))\n",
       "| StrOutputParser())], default=RunnableLambda(lambda x: 'cound not find any sentiment'))"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "branch_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "01ba1ab6-2fab-4ad6-b822-b2b281dc8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = classify_chain | branch_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "332f649b-5f58-4d02-a8a3-650877c8b36c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['feedback'], input_types={}, partial_variables={'format_instruction': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"sentiment\": {\"description\": \"Give the sentiment of the feedback\", \"enum\": [\"positive\", \"negative\"], \"title\": \"Sentiment\", \"type\": \"string\"}}, \"required\": [\"sentiment\"]}\\n```'}, template='Classify the sentiment of the following feedback text into positive or negative \\n {feedback} \\n {format_instruction}')\n",
       "| GoogleGenerativeAI(model='gemini-2.0-flash', google_api_key=SecretStr('**********'), client=ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001AD0CA72CB0>, default_metadata=(), model_kwargs={}))\n",
       "| PydanticOutputParser(pydantic_object=<class '__main__.Feedback'>)\n",
       "| RunnableBranch(branches=[(RunnableLambda(lambda x: x.sentiment == 'positive'), PromptTemplate(input_variables=['feedback'], input_types={}, partial_variables={}, template='Write an appropriate response to this positive feedback \\n {feedback}')\n",
       "  | GoogleGenerativeAI(model='gemini-2.0-flash', google_api_key=SecretStr('**********'), client=ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001AD0CA72CB0>, default_metadata=(), model_kwargs={}))\n",
       "  | StrOutputParser()), (RunnableLambda(lambda x: x.sentiment == 'negative'), PromptTemplate(input_variables=['feedback'], input_types={}, partial_variables={}, template='Write an appropriate response to this negative feedback \\n {feedback}')\n",
       "  | GoogleGenerativeAI(model='gemini-2.0-flash', google_api_key=SecretStr('**********'), client=ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001AD0CA72CB0>, default_metadata=(), model_kwargs={}))\n",
       "  | StrOutputParser())], default=RunnableLambda(lambda x: 'cound not find any sentiment'))"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b1598b2b-2e4b-4286-9294-23f7ef7a23bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    +-------------+      \n",
      "    | PromptInput |      \n",
      "    +-------------+      \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "   +----------------+    \n",
      "   | PromptTemplate |    \n",
      "   +----------------+    \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      " +--------------------+  \n",
      " | GoogleGenerativeAI |  \n",
      " +--------------------+  \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "+----------------------+ \n",
      "| PydanticOutputParser | \n",
      "+----------------------+ \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "       +--------+        \n",
      "       | Branch |        \n",
      "       +--------+        \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "    +--------------+     \n",
      "    | BranchOutput |     \n",
      "    +--------------+     \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "53991002-7a7a-44cb-bb69-d7599d65ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({'feedback': \"This is a beautiful phone\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b56b5ab8-7945-4520-9da1-a724b84b9bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Okay, here are a few response options, depending on the context and how much detail you want to provide:\\n\\n**Short & Sweet:**\\n\\n*   \"Thank you! We\\'re glad you had a positive experience.\"\\n*   \"We appreciate the positive feedback!\"\\n*   \"Thanks so much for your kind words!\"\\n\\n**Slightly More Detailed:**\\n\\n*   \"Thank you! We\\'re so happy to hear you had a positive experience. We strive to [mention what you strive for - e.g., provide excellent service, create quality products].\"\\n*   \"Thank you for the positive feedback! We really appreciate you taking the time to share your thoughts.\"\\n*   \"We\\'re delighted you enjoyed [mention the product/service if you know it]. Thank you for your positive review!\"\\n\\n**If you want to encourage further engagement:**\\n\\n*   \"Thank you! We\\'re so glad you had a positive experience. We\\'d love to hear more about what you enjoyed most!\"\\n*   \"Thanks for the positive feedback! We\\'re always looking for ways to improve. If you have any specific suggestions, please feel free to share them.\"\\n\\n**If the feedback mentioned a specific person or team:**\\n\\n*   \"Thank you! I\\'ll be sure to pass your positive feedback along to [person/team name]. They\\'ll be thrilled to hear it.\"\\n\\n**To choose the best response, consider:**\\n\\n*   **Where the feedback came from:** (e.g., a review, a comment, an email)\\n*   **How specific the feedback was:** If they mentioned something specific, acknowledge that.\\n*   **Your brand\\'s voice:** Keep your response consistent with your brand\\'s personality.\\n\\nNo matter which you choose, make sure it sounds genuine and expresses your appreciation!'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e9cfd3-bfa1-4c09-a16a-88d7790d37d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe71546-8367-4c85-90ef-57b7ddaa91dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c7ddcf-f0a7-4660-9982-ebea30ba06a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_langchain",
   "language": "python",
   "name": ".venv_langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
