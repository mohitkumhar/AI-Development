{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65beccaf-4a32-4d5a-9f98-a627b92d6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0956715d-5d39-4df1-8a65-c499f6dede0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16a26c20-8501-47bd-b28e-289e8de43129",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1_path = r\"D:\\ai\\AI-Development\\Deep Face\\dataset\\1.jpg\"\n",
    "img2_path = r\"D:\\ai\\AI-Development\\Deep Face\\dataset\\2.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e281030-f200-4464-95b2-32abbc9197da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='Facenet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00f054fd-9af1-4bc6-9fd5-20a0d15da361",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model/conv2d_13/Relu' defined at (most recent call last):\n    File \"C:\\Users\\write\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\write\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\write\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"C:\\Users\\write\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"C:\\Users\\write\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n      result = self._run_cell(\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n      result = runner(coro)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n      coro.send(None)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\write\\AppData\\Local\\Temp\\ipykernel_8452\\1399835842.py\", line 1, in <module>\n      analysis = DeepFace.analyze(\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\deepface\\DeepFace.py\", line 421, in analyze\n      gender_prediction = models['gender'].predict(img_224)[0,:]\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2253, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function\n      return step_function(self, iterator)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step\n      outputs = model.predict_step(data)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n      return self(x, training=False)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py\", line 314, in call\n      return self.activation(outputs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\activations.py\", line 317, in relu\n      return backend.relu(\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\backend.py\", line 5366, in relu\n      x = tf.nn.relu(x)\nNode: 'model/conv2d_13/Relu'\nOOM when allocating tensor with shape[4096,512,7,7] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/conv2d_13/Relu}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_predict_function_8919]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[43mDeepFace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mimg1_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgender\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdetector_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mopencv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\camera\\.camera_venv\\lib\\site-packages\\deepface\\DeepFace.py:421\u001b[0m, in \u001b[0;36manalyze\u001b[1;34m(img_path, actions, models, enforce_detection, detector_backend, prog_bar)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img_224 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    419\u001b[0m \timg_224, region \u001b[38;5;241m=\u001b[39m functions\u001b[38;5;241m.\u001b[39mpreprocess_face(img \u001b[38;5;241m=\u001b[39m img_path, target_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m), grayscale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, enforce_detection \u001b[38;5;241m=\u001b[39m enforce_detection, detector_backend \u001b[38;5;241m=\u001b[39m detector_backend, return_region \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 421\u001b[0m gender_prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgender\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_224\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m,:]\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(gender_prediction) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    424\u001b[0m \tgender \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWoman\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mf:\\camera\\.camera_venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mf:\\camera\\.camera_venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'model/conv2d_13/Relu' defined at (most recent call last):\n    File \"C:\\Users\\write\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\write\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\write\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"C:\\Users\\write\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"C:\\Users\\write\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n      result = self._run_cell(\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n      result = runner(coro)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n      coro.send(None)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\write\\AppData\\Local\\Temp\\ipykernel_8452\\1399835842.py\", line 1, in <module>\n      analysis = DeepFace.analyze(\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\deepface\\DeepFace.py\", line 421, in analyze\n      gender_prediction = models['gender'].predict(img_224)[0,:]\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2253, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function\n      return step_function(self, iterator)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step\n      outputs = model.predict_step(data)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n      return self(x, training=False)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py\", line 314, in call\n      return self.activation(outputs)\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\activations.py\", line 317, in relu\n      return backend.relu(\n    File \"f:\\camera\\.camera_venv\\lib\\site-packages\\keras\\backend.py\", line 5366, in relu\n      x = tf.nn.relu(x)\nNode: 'model/conv2d_13/Relu'\nOOM when allocating tensor with shape[4096,512,7,7] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/conv2d_13/Relu}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_predict_function_8919]"
     ]
    }
   ],
   "source": [
    "analysis = DeepFace.analyze(\n",
    "                    img1_path,\n",
    "                    actions=['gender'],\n",
    "                    detector_backend='opencv'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f299a9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Woman'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45f8c2cf-ba68-4022-beed-23588c686a72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = DeepFace.verify(img1_path=img1_path, img2_path=img2_path, model_name=model_name, detector_backend='retinaface')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "056a106c-8350-4843-af3f-49204dd7f0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verified': False,\n",
       " 'distance': 1.0036778638273478,\n",
       " 'threshold': 0.4,\n",
       " 'model': 'Facenet',\n",
       " 'detector_backend': 'retinaface',\n",
       " 'similarity_metric': 'cosine',\n",
       " 'facial_areas': {'img1': {'x': 69,\n",
       "   'y': 33,\n",
       "   'w': 70,\n",
       "   'h': 95,\n",
       "   'left_eye': (125, 71),\n",
       "   'right_eye': (91, 72)},\n",
       "  'img2': {'x': 1001,\n",
       "   'y': 411,\n",
       "   'w': 768,\n",
       "   'h': 1080,\n",
       "   'left_eye': (1642, 842),\n",
       "   'right_eye': (1295, 860)}},\n",
       " 'time': 5.07}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab88ef1e-5138-4eec-a1ff-007488e5f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - (0.2314588067803215 / 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82a8794c-98a2-46eb-a61c-e0a996b9e483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['verified'] # show similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d390780f-7f76-4fca-a84c-4b8abda7876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_face(img1_path, img2_path):\n",
    "    result = DeepFace.verify(img1_path=img1_path, img2_path=img2_path)\n",
    "    return result['verified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b473283-5460-4717-9422-0e1537f4320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(verify_face, open('verify_face.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fd2376-5ee0-4a2f-b1c9-1b7ca0bfe3cb",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f954b264-217b-4499-a0f8-feee4bae4144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbd293b0-e782-46a4-bade-d921665c617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open('verify_face.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed9fcecc-568c-4d14-b4e7-d1658aaf3bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.verify_face(img1_path, img2_path)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ab9fd1f-ab8d-4e07-bcf1-759d3362ee93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['verified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2e641f-71e0-47e5-8db6-c017716cd855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".camera_venv",
   "language": "python",
   "name": ".camera_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
