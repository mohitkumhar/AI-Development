{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec5445b-ce9c-4867-a576-16b72f7cffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create only one trace of all the process unlike before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "166bae30-1bfc-4e7f-8e92-5f4c7ea34179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U langchain langchain-openai langchain-community faiss-cpu pypdf python-dotenv langsmith\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langsmith import traceable\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deaabbce-683b-44c7-bef8-71e124faa7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_PROJECT'] = \"RAG Chatbot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "087bc970-0f81-4349-9e05-3d347ca35035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae12b559-26c4-4ab5-9759-c25c9d284f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = \"resume.pdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7ec9f2c-47a9-4c45-a5dd-8dafea2ec538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- helpers (not traced individually) -----------------\n",
    "@traceable(name=\"load_pdf\")\n",
    "def load_pdf(path: str):\n",
    "    loader = PyPDFLoader(path)\n",
    "    return loader.load()  # list[Document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b14e7eee-54ef-4794-b24a-8b734ae51693",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(name=\"split_documents\")\n",
    "def split_documents(docs, chunk_size=1000, chunk_overlap=150):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cc9f92c-ac5a-4170-898b-3b4992cc30c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(name=\"build_vectorstore\")\n",
    "def build_vectorstore(splits):\n",
    "    emb = OpenAIEmbeddings(model=\"text-embedding-3-small\", base_url=\"https://openrouter.ai/api/v1\")\n",
    "    return FAISS.from_documents(splits, emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb733e11-4c2d-461f-a82c-2b9a58962089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- parent setup function (traced) -----------------\n",
    "@traceable(name=\"setup_pipeline\", tags=[\"setup\"])\n",
    "def setup_pipeline(pdf_path: str, chunk_size=1000, chunk_overlap=150):\n",
    "    # ✅ These three steps are “clubbed” under this parent function\n",
    "    docs = load_pdf(pdf_path)\n",
    "    splits = split_documents(docs, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    vs = build_vectorstore(splits)\n",
    "    return vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "925c19b5-60ae-4141-b992-243d0a4f5411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- model, prompt, and run -----------------\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"https://openrouter.ai/api/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98f9a0b4-58dc-4bf7-8e17-dfa0b3be525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer ONLY from the provided context. If not found, say you don't know.\"),\n",
    "    (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "430fe0e8-0337-4936-9f78-f19da92cf16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80ba8651-9db2-4e67-9962-57ab22875e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- one top-level (root) run -----------------\n",
    "@traceable(name=\"pdf_rag_full_run\")\n",
    "def setup_pipeline_and_query(pdf_path: str, question: str):\n",
    "    # Parent setup run (child of root)\n",
    "    vectorstore = setup_pipeline(pdf_path, chunk_size=1000, chunk_overlap=150)\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "    parallel = RunnableParallel({\n",
    "        \"context\": retriever | RunnableLambda(format_docs),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    })\n",
    "\n",
    "    chain = parallel | prompt | llm | StrOutputParser()\n",
    "\n",
    "    # This LangChain run stays under the same root (since we're inside this traced function)\n",
    "    lc_config = {\"run_name\": \"pdf_rag_query\"}\n",
    "    return chain.invoke(question, config=lc_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a41fab64-9182-412c-8a14-c093f28cd3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF RAG ready. Ask a question (or Ctrl+C to exit).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Q:  my skills\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A: Your skills include:\n",
      "\n",
      "- **Programming Languages**: Python, C/C++\n",
      "- **Data Manipulation**: Pandas, NumPy, Data Wrangling, Data Cleaning, Feature Engineering\n",
      "- **Data Visualization**: Matplotlib, Seaborn, Plotly\n",
      "- **Machine Learning**: Scikit-learn, Model Deployment, Model Evaluation\n",
      "- **Natural Language Processing (NLP)**: Text Preprocessing, Sentiment Analysis, Named Entity Recognition (NER), Word Embeddings (Word2Vec), SpaCy\n",
      "- **Model Deployment**: Flask API, Docker, Heroku, Vercel, Streamlit, Netlify\n",
      "- **Data Stores**: MySQL, PostgreSQL, MongoDB\n",
      "- **Version Control**: Git, GitHub, GitLab\n",
      "- **Operating Systems**: Windows, Linux (Ubuntu, Kali Linux), macOS\n",
      "- **Tools & Libraries**: Selenium, BeautifulSoup, Jupyter Notebooks, VS Code, Colab\n",
      "\n",
      "Additionally, you have experience in building intelligent systems, real-time monitoring applications, and working with machine learning models and data analysis. You have also participated in various hackathons and events, showcasing your problem-solving skills and engagement in the tech community.\n"
     ]
    }
   ],
   "source": [
    "# ----------------- CLI -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"PDF RAG ready. Ask a question (or Ctrl+C to exit).\")\n",
    "    q = input(\"\\nQ: \").strip()\n",
    "    ans = setup_pipeline_and_query(PDF_PATH, q)\n",
    "    print(\"\\nA:\", ans)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf6e3d6-8e36-467e-a21b-8775a639fe52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
