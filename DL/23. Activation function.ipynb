{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce1f2d73-5b9f-43cd-bbc1-ef31e1de1b79",
   "metadata": {},
   "source": [
    "- activation function is used to create non linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1677c30-695a-4931-83a1-171f6fef6a8a",
   "metadata": {},
   "source": [
    "- Without activation function the model will always trained on a linear behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e187a87-8be4-4111-bb8b-91f15ae34882",
   "metadata": {},
   "source": [
    "- - training will faster (faster converge) if the input is normalize\n",
    "  - example - tanh (activation function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d97ac1-53da-41f9-a095-6fbb7136e383",
   "metadata": {},
   "source": [
    "activation function must be non saturating \n",
    "- LIKE \n",
    "    - relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec34714-d75f-41bd-a624-326621814a09",
   "metadata": {},
   "source": [
    "- example of saturating function are: \n",
    "    - sigmoid (it squeese input in range (0 , 1))\n",
    "    - tanh (it squeese input in range(-1, 1))\n",
    "\n",
    "- saturating function have more chances of getting <b>Vanishing Gradient Problem</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f117b6-0d9b-45ff-87c2-3278bd508373",
   "metadata": {},
   "source": [
    "# Activation Functions in Neural Networks  \n",
    "\n",
    "Activation functions decide whether a neuron should be activated or not. They help introduce non-linearity, allowing neural networks to learn complex patterns. Three common activation functions are **Sigmoid, ReLU, and Tanh**.  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Sigmoid Activation Function  \n",
    "**Formula:**  \n",
    "\\[\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\]  \n",
    "\n",
    "**Range:** `0` to `1`  \n",
    "\n",
    "**When to Use:**  \n",
    "- Used in **binary classification** problems (output between 0 and 1).  \n",
    "- Good for **probabilities** (e.g., logistic regression).  \n",
    "- **Disadvantage:** Causes **vanishing gradient problem** (very small changes in weights for large or small inputs).  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. ReLU (Rectified Linear Unit) Activation Function  \n",
    "**Formula:**  \n",
    "\\[\n",
    "f(x) = \\max(0, x)\n",
    "\\]  \n",
    "\n",
    "**Range:** `0` to `âˆž`  \n",
    "\n",
    "**When to Use:**  \n",
    "- Used in **hidden layers** of deep neural networks.  \n",
    "- Faster training because it doesnâ€™t saturate like sigmoid or tanh.  \n",
    "- **Disadvantage:** **Dying ReLU problem** (neurons can become inactive for all inputs if they get stuck at 0).  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Tanh (Hyperbolic Tangent) Activation Function  \n",
    "**Formula:**  \n",
    "\\[\n",
    "f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "\\]  \n",
    "\n",
    "**Range:** `-1` to `1`  \n",
    "\n",
    "**When to Use:**  \n",
    "- Used in **hidden layers** when values need to be centered around 0.  \n",
    "- Works better than **sigmoid** for some deep networks.  \n",
    "- **Disadvantage:** Also suffers from the **vanishing gradient problem** like sigmoid.  \n",
    "\n",
    "---\n",
    "\n",
    "## Which Activation Function to Use?  \n",
    "\n",
    "| **Use Case**             | **Best Activation Function** |\n",
    "|-------------------------|-------------------------|\n",
    "| Binary classification (output layer) | Sigmoid |\n",
    "| Hidden layers in deep networks | ReLU |\n",
    "| When values should be centered around 0 | Tanh |\n",
    "| Preventing vanishing gradient in deep networks | ReLU |\n",
    "\n",
    "Let me know if you need more details! ðŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecb0b2b-9d6b-475f-a631-ace7011181d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
